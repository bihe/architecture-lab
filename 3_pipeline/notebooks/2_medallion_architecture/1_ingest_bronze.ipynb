{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "120a4ec7-7e2c-4dbd-82d0-ac31c3dcd8e9",
   "metadata": {},
   "source": [
    "# Bronze: Ingest data\n",
    "The bronze lazer of the [medallion architecture](https://www.databricks.com/glossary/medallion-architecture) is mainly responsible to ingest data from different sources.\n",
    "The logic below reads \"streaming\" data via a Kafka Topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9bf46c-c29c-4820-b971-142ce625b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka defintions\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "KAFKA_BROKER = \"kafka:9092\"\n",
    "TOPIC = \"weather-data\"\n",
    "GROUP_ID = \"weather-data-bronze-loop\" # change the group-id to ingest the whole data again\n",
    "\n",
    "# MinIO Configuration\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "# Danger-Zone (https://www.youtube.com/watch?v=siwpn14IE7E)\n",
    "# Typicalla a kind of vault would be used e.g. https://azure.microsoft.com/en-us/products/key-vault, ...\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "BUCKET_NAME = \"weather-data\"\n",
    "\n",
    "# Weather API\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "FORECAST_DAYS=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89548f98-a203-43fc-a6c2-261af5f65d02",
   "metadata": {},
   "source": [
    "## Query the API and publish to the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05cf53f-fbe3-4575-8ace-e99afb39f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Initialize Kafka producer\n",
    "producer = Producer({'bootstrap.servers': KAFKA_BROKER})\n",
    "\n",
    "# Callback to confirm delivery\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Sent: {msg.value().decode('utf-8')}\")\n",
    "\n",
    "# Function to fetch weather data\n",
    "def fetch_weather_data():\n",
    "    url = 'https://api.open-meteo.com/v1/forecast'\n",
    "    params = {\n",
    "        'latitude': 47.72,  # Puch Urstein!\n",
    "        'longitude': 13.09, # Puch Urstein!\n",
    "        'hourly': 'temperature_2m,relative_humidity_2m,wind_speed_10m',\n",
    "        'timezone': 'UTC',\n",
    "        'forecast_days': FORECAST_DAYS # look into the future\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "# Fetch and send data to Kafka\n",
    "weather_data = fetch_weather_data()\n",
    "\n",
    "hourly_data = weather_data['hourly']\n",
    "for i, timestamp in enumerate(hourly_data['time']):\n",
    "    record = {\n",
    "        'timestamp': timestamp,\n",
    "        'temperature': hourly_data['temperature_2m'][i],\n",
    "        'humidity': hourly_data['relative_humidity_2m'][i],\n",
    "        'wind_speed': hourly_data['wind_speed_10m'][i]\n",
    "    }\n",
    "    payload = json.dumps(record).encode('utf-8')\n",
    "    producer.produce(TOPIC, value=payload, callback=delivery_report)\n",
    "    producer.poll(0)  # Trigger delivery report callbacks\n",
    "    time.sleep(1)  # Simulate real-time data streaming\n",
    "\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf23b2d7-381d-4b38-8929-36f0c0a4fb1c",
   "metadata": {},
   "source": [
    "## Consume from Kafka topic\n",
    "The approch is to \"listen\" to data in the queue in batch-sizes and store the data in [parquet](https://parquet.apache.org/) files on the storage layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9df1e4-24ec-4a16-8d39-0bd9085bbb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import BytesIO\n",
    "import datetime\n",
    "import boto3\n",
    "\n",
    "\n",
    "# Initialize Kafka Consumer\n",
    "consumer = Consumer({\n",
    "    'bootstrap.servers': KAFKA_BROKER,\n",
    "    'group.id': GROUP_ID,\n",
    "    'auto.offset.reset': 'earliest'\n",
    "})\n",
    "consumer.subscribe([TOPIC])\n",
    "\n",
    "# Initialize MinIO Client\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "s3_resource = boto3.resource('s3', \n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "     aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "# Continuous ingestion loop\n",
    "print(\"Listening for messages...\")\n",
    "batch = []\n",
    "\n",
    "# ensure storage bucket is available\n",
    "def init_bucket(bucket_name):\n",
    "    create_bucket=True\n",
    "    for b in s3_resource.buckets.all():\n",
    "        if b.name == bucket_name:\n",
    "            create_bucket = False\n",
    "    if create_bucket is True:\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "\n",
    "init_bucket(BUCKET_NAME)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print(f\"Error: {msg.error()}\")\n",
    "            continue\n",
    "\n",
    "        record = json.loads(msg.value().decode(\"utf-8\"))\n",
    "        print(f\"Received: {record}\")\n",
    "        batch.append(record)\n",
    "\n",
    "        # Flush batch every 24 records\n",
    "        if len(batch) >= 24:\n",
    "            df = pd.DataFrame(batch)\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format='mixed', utc=True)\n",
    "\n",
    "            # Partition by current UTC date\n",
    "            now = datetime.datetime.now(datetime.timezone.utc)\n",
    "            current_date = now.strftime(\"%Y-%m-%d\")\n",
    "            parquet_key = f\"bronze/{current_date}/weather_{now.timestamp()}.parquet\"\n",
    "\n",
    "            # Save to MinIO\n",
    "            buffer = BytesIO()\n",
    "            df.to_parquet(buffer, engine=\"pyarrow\", index=False)\n",
    "            \n",
    "            s3_client.put_object(Bucket=BUCKET_NAME, Key=parquet_key, Body=buffer.getvalue())\n",
    "            print(f\"Stored {len(batch)} records to MinIO at: {parquet_key}\")\n",
    "\n",
    "            batch = []\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping consumer...\")\n",
    "\n",
    "finally:\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8dadfd-ae0d-466b-b709-5efeb9b27f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
